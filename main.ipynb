{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MUHAMMAD\n",
      "[nltk_data]     AHMAD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "d:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\python\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading Sentence Transformer model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import nltk\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from flask import Flask, request, jsonify\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# File paths\n",
    "STORAGE_FILE = \"scraped_content.txt\"\n",
    "FAISS_INDEX_FILE = \"faiss_indexx.bin\"\n",
    "EMBEDDINGS_FILE = \"embeddingss.npy\"\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "--show-config-json\n",
      "    Show the application's configuration (json format)\n",
      "    Equivalent to: [--Application.show_config_json=True]\n",
      "--generate-config\n",
      "    generate default config file\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only\n",
      "            relevant when converting to notebook format)\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
      "--clear-output\n",
      "    Clear output of current file and save in place,\n",
      "            overwriting the existing notebook.\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
      "--coalesce-streams\n",
      "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document.\n",
      "            This mode is ideal for generating code-free reports.\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
      "--allow-chromium-download\n",
      "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
      "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
      "--disable-chromium-sandbox\n",
      "    Disable chromium security sandbox when converting to PDF..\n",
      "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
      "--show-input\n",
      "    Shows code input. This flag is only useful for dejavu users.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
      "--embed-images\n",
      "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
      "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
      "--sanitize-html\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
      "--log-level=<Enum>\n",
      "    Set the log level by value or name.\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
      "    Default: 30\n",
      "    Equivalent to: [--Application.log_level]\n",
      "--config=<Unicode>\n",
      "    Full path of a config file.\n",
      "    Default: ''\n",
      "    Equivalent to: [--JupyterApp.config_file]\n",
      "--to=<Unicode>\n",
      "    The export format to be used, either one of the built-in formats\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
      "            or a dotted object name that represents the import path for an\n",
      "            ``Exporter`` class\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.export_format]\n",
      "--template=<Unicode>\n",
      "    Name of the template to use\n",
      "    Default: ''\n",
      "    Equivalent to: [--TemplateExporter.template_name]\n",
      "--template-file=<Unicode>\n",
      "    Name of the template file to use\n",
      "    Default: None\n",
      "    Equivalent to: [--TemplateExporter.template_file]\n",
      "--theme=<Unicode>\n",
      "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
      "    as prebuilt extension for the lab template)\n",
      "    Default: 'light'\n",
      "    Equivalent to: [--HTMLExporter.theme]\n",
      "--sanitize_html=<Bool>\n",
      "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
      "    should be set to True by nbviewer or similar tools.\n",
      "    Default: False\n",
      "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
      "--writer=<DottedObjectName>\n",
      "    Writer class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: 'FilesWriter'\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\n",
      "--post=<DottedOrNone>\n",
      "    PostProcessor class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
      "--output=<Unicode>\n",
      "    Overwrite base name use for output files.\n",
      "                Supports pattern replacements '{notebook_name}'.\n",
      "    Default: '{notebook_name}'\n",
      "    Equivalent to: [--NbConvertApp.output_base]\n",
      "--output-dir=<Unicode>\n",
      "    Directory to write output(s) to. Defaults\n",
      "                                  to output to the directory of each notebook. To recover\n",
      "                                  previous default behaviour (outputting to the current\n",
      "                                  working directory) use . as the flag value.\n",
      "    Default: ''\n",
      "    Equivalent to: [--FilesWriter.build_directory]\n",
      "--reveal-prefix=<Unicode>\n",
      "    The URL prefix for reveal.js (version 3.x).\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
      "            of reveal.js.\n",
      "            For speaker notes to work, this must be a relative path to a local\n",
      "            copy of reveal.js: e.g., \"reveal.js\".\n",
      "            If a relative path is given, it must be a subdirectory of the\n",
      "            current directory (from which the server is run).\n",
      "            See the usage documentation\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
      "            for more details.\n",
      "    Default: ''\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
      "--nbformat=<Enum>\n",
      "    The nbformat version to write.\n",
      "            Use this to downgrade notebooks.\n",
      "    Choices: any of [1, 2, 3, 4]\n",
      "    Default: 4\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to html\n",
      "\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
      "\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
      "            'classic'. You can specify the flavor of the format used.\n",
      "\n",
      "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
      "\n",
      "            You can also pipe the output to stdout, rather than a file\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "\n",
      "            PDF is generated via latex\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\n",
      "\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "\n",
      "            Multiple notebooks can be given at the command line in a couple of\n",
      "            different ways:\n",
      "\n",
      "            > jupyter nbconvert notebook*.ipynb\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "\n",
      "            or you can specify the notebooks list in a config file, containing::\n",
      "\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "\n",
      "            > jupyter nbconvert --config mycfg.py\n",
      "\n",
      "To see all available configurables, use `--help-all`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to main.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETINGS = [\n",
    "    \"Hey there! üòä\",  \n",
    "    \"Hello! How are you Today? üòä\",  \n",
    "    \"Hi! Hope you're doing great.\",  \n",
    "    \"Hey! Let‚Äôs explore your questions together üöÄ\",  \n",
    "    \"Hi there! üí°\",  \n",
    "    \"Hello! Hope your day is going well. ‚ú®\",  \n",
    "    \"Heyüîç\",\n",
    "    \"Hi!üëç\",  \n",
    "    \"Hey there! Ready for a quick assistance üòä\",  \n",
    "    \"Hello! Let‚Äôs dive in! üèÜ\",  \n",
    "    \"Hi, I'm Todd, your friendly assistant üòä\", \n",
    "    \"Hi there! I'm good üòä\",  \n",
    "    \"Doing great! How can I help? üëç\",  \n",
    "    \"I'm good, thanks for asking! What about you?\",  \n",
    "    \"Nice to meet you! How can I assist? ü§ñ\",  \n",
    "    \"Hope you're having a great day! ‚òÄÔ∏è\",  \n",
    "    \"Hey! What's new with you? üöÄ\",  \n",
    "    \"It's great to see you! üòä\",  \n",
    "]\n",
    "\n",
    "CLOSINGS = [\n",
    "    \"Hope this helps! Let me know if you need anything else. üöÄ\",\n",
    "    \"That's all for now! Let me know if you have more questions. üòä\",\n",
    "    \"Feel free to ask if you need further clarification. Have a great day! üéâ\",\n",
    "    \"Let me know if I can assist you with anything else! ‚ú®\",\n",
    "    \"I hope this makes things clearer. Reach out if you need more help! üëç\",\n",
    "    \"If you need any more details, just let me know! üòä\",\n",
    "    \"That‚Äôs it for now! Hope you have a great day ahead. üåü\",\n",
    "    \"Happy to help! Let me know if you need more info. üéØ\",\n",
    "    \"Hope this answered your question! Feel free to ask anything else. üîç\",\n",
    "    \"Take care and let me know if you need anything else! üòä\"\n",
    "]\n",
    "\n",
    "KEYWORD_EMOJI_MAP = {\n",
    "    \"click\": \"üñ±Ô∏è\", \"select\": \"‚úÖ\", \"navigate\": \"üß≠\", \"update\": \"üîÑ\", \"manage\": \"üìã\",\n",
    "    \"open\": \"üìÇ\", \"go to\": \"‚û°Ô∏è\", \"press\": \"üîò\", \"drag\": \"üñ±Ô∏è\", \"drop\": \"üì•\",\n",
    "    \"choose\": \"üéØ\", \"install\": \"‚öôÔ∏è\", \"enable\": \"üîõ\", \"disable\": \"üö´\",\n",
    "    \"configure\": \"üîß\", \"customize\": \"üé®\"\n",
    "}\n",
    "\n",
    "APPRECIATION_RESPONSES = [\n",
    "    \"You're welcome! üòä Let me know if you need anything else.\",\n",
    "    \"Glad I could help! üöÄ\",\n",
    "    \"Happy to assist! üéØ\",\n",
    "    \"No problem! Let me know if you have more questions. üëç\",\n",
    "    \"Always here to help! üòä\",\n",
    "    \"Thanks for your kind words! Let me know if you need more info. üí°\",\n",
    "    \"Much appreciated! If you need anything else, feel free to ask! ‚ú®\"\n",
    "]\n",
    "HELP_RESPONSE = [\n",
    "    # Friendly & Encouraging Responses\n",
    "    \"Sure!! What do you need help with? üòä\",\n",
    "    \"Of course! I'm happy to help. What's your question? üöÄ\",\n",
    "    \"No worries! Tell me what you need help with, and I'll do my best! ‚ú®\",\n",
    "    \"Absolutely! Let me know what you're stuck on. I'll guide you. üí°\",\n",
    "    \"You're not alone! I'm here to help. What do you need assistance with? üîç\",\n",
    "\n",
    "    # Professional & Supportive Responses\n",
    "    \"I'm here to assist. Please describe your issue, and I'll help you solve it. üëç\",\n",
    "    \"I'm happy to support you. Could you provide more details about your question? ü§î\",\n",
    "    \"I‚Äôd be glad to help! Let me know what you're struggling with. üéØ\",\n",
    "    \"No problem! Just let me know how I can assist you today. üîß\",\n",
    "    \"I understand! Please share the details, and I'll do my best to help. ‚úÖ\",\n",
    "\n",
    "    # Fast & Reassuring Responses\n",
    "    \"Got it! Let‚Äôs tackle this together. What do you need help with? üí™\",\n",
    "    \"I see! Let me simplify things for you. Just tell me what you need. üìù\",\n",
    "    \"Helping is what I do best! What‚Äôs on your mind? üéâ\",\n",
    "    \"I hear you! Let‚Äôs find the best solution together. Tell me more. üîé\",\n",
    "    \"Sure thing! Let me guide you step by step. What‚Äôs the issue? üë®‚Äçüè´\",\n",
    "\n",
    "    # Problem-Solving & Guidance-Based Responses\n",
    "    \"Don‚Äôt worry, we‚Äôll figure this out! Just give me the details. üöÄ\",\n",
    "    \"Let's get this sorted! Tell me what's going on, and I'll help. üéØ\",\n",
    "    \"I‚Äôm ready to assist! What specific issue are you facing? üõ†\",\n",
    "    \"Break it down for me! I‚Äôll help you get to the solution. üß©\",\n",
    "    \"Happy to help! Let's find the best way forward. What‚Äôs the challenge? ‚ö°\",\n",
    "\n",
    "    # Empathetic & Motivational Responses\n",
    "    \"I know how frustrating this can be! Let‚Äôs work through it together. üåü\",\n",
    "    \"Take your time! I‚Äôll be here to help whenever you‚Äôre ready. ‚è≥\",\n",
    "    \"You're doing great! Let‚Äôs make this easier. What do you need? üöÄ\",\n",
    "    \"I totally get it! Let‚Äôs get this sorted right away. üî•\",\n",
    "    \"You're not alone in this! Let's solve it step by step. üèÜ\"\n",
    "]\n",
    "\n",
    "GENERIC_GREETINGS = {\n",
    "    # ‚úÖ Common English greetings (Mapped to Multiple Responses)\n",
    "    (\"hi\", \"hello\", \"hey\", \"hey there\", \"hi there\", \"greetings\", \"hello there\"): [0, 2, 8, 16], \n",
    "    (\"good morning\", \"good afternoon\", \"good evening\", \"good day\"): [5, 15, 17],  \n",
    "\n",
    "    # ‚úÖ \"How are you?\" Variations (Multiple Responses)\n",
    "    (\"how are you\", \"how‚Äôs it going\", \"how are you doing\", \"how do you do\", \n",
    "     \"how have you been\", \"how‚Äôs your day\", \"how‚Äôs life\", \"how‚Äôs everything\"): [3, 11, 12, 13],  \n",
    "\n",
    "    # ‚úÖ Casual slang & internet greetings (Mapped to Multiple Responses)\n",
    "    (\"wassup\", \"whatsup\", \"sup\", \"sup bro\", \"sup dude\", \"yo\", \"hiya\", \"holla\", \n",
    "     \"hey mate\", \"yo yo\", \"what‚Äôs good\", \"what‚Äôs new\", \"hey fam\", \"hey bro\", \"hey sis\", \"how‚Äôs it hanging\"): [6, 7, 16],  \n",
    "\n",
    "    # ‚úÖ Friendly reunion greetings (Mapped to Multiple Responses)\n",
    "    (\"long time no see\", \"hey friend\", \"hi buddy\", \"hello friend\", \"nice to meet you\", \"pleased to meet you\"): [8, 14, 17],  \n",
    "\n",
    "    # ‚úÖ Formal greetings (Mapped to Multiple Responses)\n",
    "    (\"good to see you\", \"hope you‚Äôre doing well\", \"it's a pleasure to meet you\"): [4, 5, 14],  \n",
    "\n",
    "    # ‚úÖ Multilingual greetings (Mapped to Multiple Responses)\n",
    "    (\"hola\", \"bonjour\", \"ciao\", \"shalom\", \"salam\", \"aloha\", \"namaste\", \"konnichiwa\", \n",
    "     \"annyeong\", \"ni hao\", \"guten tag\", \"privet\", \"zdravstvuyte\", \"merhaba\", \"sawubona\", \"vanakkam\", \"yassas\"): [1, 9, 15],  \n",
    "\n",
    "    # ‚úÖ Todd Introductions (Mapped to Multiple Responses)\n",
    "    (\"who are you\", \"what are you\", \"introduce yourself\", \"tell me about yourself\"): [10]  \n",
    "}\n",
    "\n",
    "\n",
    "APPRECIATION_KEYWORDS = [\n",
    "    # Common appreciation phrases\n",
    "    \"thanks\", \"ok\", \"thank you\", \"appreciate it\", \"great work\", \"nice job\",\n",
    "    \"well done\", \"awesome\", \"amazing\", \"good job\", \"fantastic\", \"love it\",\n",
    "    \"keep it up\", \"kudos\", \"much appreciated\", \"hats off\", \"respect\",\n",
    "    \n",
    "    # Expressing gratitude casually\n",
    "    \"thanks a lot\", \"many thanks\", \"thanks so much\", \"thanks a ton\",\n",
    "    \"thanks a bunch\", \"cheers\", \"big thanks\", \"huge thanks\", \"massive thanks\",\n",
    "    \"thanks buddy\", \"thanks bro\", \"thank you so much\", \"thank you tons\",\n",
    "    \n",
    "    # Formal expressions of gratitude\n",
    "    \"I truly appreciate it\", \"I'm grateful\", \"much obliged\", \"I'm in your debt\",\n",
    "    \"thank you kindly\", \"I can't thank you enough\", \"eternally grateful\",\n",
    "    \"sincere thanks\", \"profound gratitude\", \"heartfelt thanks\",\n",
    "\n",
    "    # Internet slang/modern appreciation\n",
    "    \"ty\", \"tysm\", \"thx\", \"thnx\", \"gracias\", \"danke\", \"merci\", \"arigato\",\n",
    "    \"shukran\", \"shukriya\", \"obrigado\", \"grazie\", \"dhanyavad\", \"takk\",\n",
    "    \"you rock\", \"you're the best\", \"big fan\", \"mad respect\", \"goat\",\n",
    "    \n",
    "    # Compliments & positive feedback\n",
    "    \"amazing job\", \"superb work\", \"phenomenal\", \"excellent work\",\n",
    "    \"brilliant work\", \"outstanding effort\", \"exceptional\", \"top-notch\",\n",
    "    \"terrific\", \"impressive\", \"legendary\", \"mind-blowing\"\n",
    "]\n",
    "HELP_KEYWORDS = [\n",
    "    # Direct Help Requests\n",
    "    \"help\", \"please help\", \"can you help me\", \"help me\", \"assist me\", \n",
    "    \"i need help\", \"i need assistance\", \"can you assist me\", \"help needed\",\n",
    "    \n",
    "    # Casual Help Requests\n",
    "    \"i'm stuck\", \"stuck here\", \"i can't figure this out\", \"i don't get it\",\n",
    "    \"help me out\", \"need guidance\", \"can you support\", \"support needed\",\n",
    "    \"i'm confused\", \"i need some guidance\", \"can you explain this\",\n",
    "\n",
    "    # Formal/Professional Help Requests\n",
    "    \"i have an issue\", \"i have a problem\", \"can you clarify\", \"need clarification\",\n",
    "    \"i need your help\", \"can i ask something\", \"i have a query\", \n",
    "    \"can i ask a question\", \"i need some advice\", \"can you provide guidance\",\n",
    "    \"can you help me understand this\", \"can you shed some light on this\",\n",
    "\n",
    "    # Task-Specific Help Requests\n",
    "    \"how do i do this\", \"how do i use this\", \"how do i solve this\", \n",
    "    \"what do i do next\", \"what should i do\", \"i'm not sure what to do\",\n",
    "    \"i don't know how to proceed\", \"how do i proceed\", \n",
    "    \"explain this to me\", \"guide me through this\",\n",
    "\n",
    "    # Polite Help Requests\n",
    "    \"could you help me\", \"would you mind helping me\", \"i'd appreciate your help\",\n",
    "    \"may i ask for help\", \"kindly assist me\", \"please assist me\",\n",
    "    \"i would like some help\", \"i'm seeking guidance\", \"can you lend a hand\"\n",
    "]\n",
    "# ‚úÖ Define Query Synonym Mapping\n",
    "QUERY_SYNONYMS = {\n",
    "    \"student\": [\"kid\", \"child\", \"new student\", \"pupil\"],\n",
    "    \"add\": [\"register\", \"enroll\", \"create\"],\n",
    "    \"remove\": [\"delete\", \"erase\", \"unregister\"],\n",
    "    \"access\": [\"view\",\"go to\"],\n",
    "    \"update\": [\"edit\", \"modify\", \"change\"],\n",
    "    \"password\": [\"credentials\", \"passcode\", \"login key\"],\n",
    "    \"staff\": [\"instructor\", \"educator\", \"employee\",\"worker\"],    \n",
    "    # Dashboard-related\n",
    "    \"dashboard\": [\"panel\", \"control center\", \"admin panel\", \"overview\", \"interface\"],\n",
    "    \"company dashboard\": [\"business panel\", \"corporate overview\", \"organization dashboard\"],\n",
    "    # Attendance-related\n",
    "    \"attendance\": [\"presence\", \"check-in\", \"roll call\", \"participation\", \"time tracking\"],\n",
    "    # Branch-related\n",
    "    \"branch\": [\"division\", \"unit\", \"location\", \"office\", \"subdivision\"],\n",
    "    # Classroom-related\n",
    "    \"classroom\": [\"lecture hall\", \"learning space\", \"study room\", \"training room\"],\n",
    "    # Promotion-related\n",
    "    \"promotion\": [\"advancement\", \"upgrade\", \"progression\", \"elevation\", \"boost\"],\n",
    "    # Admission Query-related\n",
    "    \"admission query\": [\"enrollment request\", \"application inquiry\", \"registration query\", \"student admission\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Success\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index and embeddings once to optimize performance\n",
    "if os.path.exists(\"faiss_indexx.bin\") and os.path.exists(\"embeddingss.npy\"):\n",
    "    index = faiss.read_index(\"faiss_indexx.bin\")\n",
    "    content = np.load(\"embeddingss.npy\", allow_pickle=True)\n",
    "    print(\"Load Success\")\n",
    "else:\n",
    "    index = None\n",
    "    content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "{greeting}\n",
    "\n",
    "    {summary}\n",
    "üîë Key Steps:\n",
    "{key_steps}\n",
    "\n",
    "{closing}\n",
    "\"\"\"\n",
    "first_query = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_summary_text(text):\n",
    "    \"\"\"Cleans and refines summary text by removing unnecessary words and rewording.\"\"\"\n",
    "    text = re.sub(r'\\b(that|which|however|thus|therefore|hence|additionally|moreover|furthermore|consequently|nevertheless)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(for example|such as|including|like)\\b', 'e.g.', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def extract_key_sentences(text, num_sentences=3):\n",
    "    \"\"\"Extracts key sentences using TF-IDF importance scoring and position-based ranking.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences) \n",
    "\n",
    "    # ‚úÖ Compute TF-IDF scores for words\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A1  \n",
    "\n",
    "    # ‚úÖRank\n",
    "    ranked_sentences = sorted(\n",
    "        enumerate(sentence_scores), key=lambda x: (x[1], -x[0]), reverse=True\n",
    "    )\n",
    "\n",
    "    # ‚úÖTop\n",
    "    best_sentences = [sentences[idx] for idx, _ in ranked_sentences[:num_sentences]]\n",
    "    return \" \".join(best_sentences)\n",
    "\n",
    "def generate_summary(text, num_sentences=3):\n",
    "    \"\"\"Generates an improved summary using LSA summarization.\"\"\"\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "\n",
    "    summary_sentences = summarizer(parser.document, num_sentences)\n",
    "    summary_text = \" \".join(str(sentence) for sentence in summary_sentences)\n",
    "\n",
    "    return summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_repeated_chars(text):\n",
    "    \"\"\"Reduces excessive character repetition but allows up to 3 consecutive occurrences.\"\"\"\n",
    "    return re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', text) \n",
    "\n",
    "def safe_extract_one(text, choices):\n",
    "    \"\"\"Handles cases where fuzzy matching might return None.\"\"\"\n",
    "    result = process.extractOne(text, choices, scorer=fuzz.ratio)\n",
    "    return result if result else (\"\", 0)  # Avoid unpacking error\n",
    "\n",
    "def extract_greeting_and_question(query):\n",
    "    \"\"\"Detects if the full query is a greeting, appreciation, or an actual question using fuzzy matching.\"\"\"\n",
    "\n",
    "    # ‚úÖ Remove punctuation and convert to lowercase for better matching\n",
    "    query_cleaned = re.sub(r'[^\\w\\s]', '', query.lower().strip())\n",
    "\n",
    "    # ‚úÖ Normalize\n",
    "    query_cleaned = reduce_repeated_chars(query_cleaned)\n",
    "\n",
    "    words = word_tokenize(query_cleaned)\n",
    "    full_query = \" \".join(words) \n",
    "    \n",
    "    \n",
    "    # ‚úÖClosest Match\n",
    "    best_greeting_match, greeting_score = safe_extract_one(full_query, [item for subset in GENERIC_GREETINGS.keys() for item in subset])\n",
    "    best_appreciation_match, appreciation_score = process.extractOne(full_query, APPRECIATION_KEYWORDS, scorer=fuzz.ratio)\n",
    "    best_help_match, help_score = process.extractOne(full_query, HELP_KEYWORDS, scorer=fuzz.ratio)\n",
    "\n",
    "    # ‚úÖGreeting\n",
    "    if greeting_score >= 70:\n",
    "        return best_greeting_match.capitalize(), None  \n",
    "\n",
    "    # ‚úÖAppreciation\n",
    "    if appreciation_score >= 70:\n",
    "        return \"Appreciation\", None  \n",
    "    \n",
    "    if help_score >= 70:\n",
    "        return \"Help\", None\n",
    "\n",
    "    # ‚úÖMixed Greeting\n",
    "    for phrase_group in GENERIC_GREETINGS.keys():\n",
    "        for phrase in phrase_group:\n",
    "            if full_query.startswith(phrase + \" \"): \n",
    "                greeting = phrase.capitalize()\n",
    "                # ‚úÖ Remove greeting from query\n",
    "                cleaned_query = full_query[len(phrase):].strip()  \n",
    "                 # ‚úÖ Return cleaned query or None\n",
    "                return greeting, cleaned_query if cleaned_query else None \n",
    "\n",
    "    return None, query \n",
    "\n",
    "def get_greeting_response(user_input):\n",
    "    \"\"\"Returns an appropriate greeting response based on user input.\"\"\"\n",
    "    user_input = user_input.lower()\n",
    "\n",
    "    for key_set, response in GENERIC_GREETINGS.items():\n",
    "        if user_input in key_set:\n",
    "            # ‚úÖ Picks random index, fetches from GREETINGS\n",
    "            return GREETINGS[random.choice(response)]  \n",
    "\n",
    "    return \"Hey! How can I assist you today? üòä\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Removes extra spaces, fixes phrasing, and makes text more readable.\"\"\"\n",
    "    # Remove extra spaces/newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    text = re.sub(r'\\bprovides quick access to\\b', 'lets you quickly access', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\ballows managing\\b', 'helps manage', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bclick on\\b', 'select', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def extract_sentences(text):\n",
    "    \"\"\"Splits text into meaningful sentences.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Proper sentence splitting\n",
    "    return [s.strip() for s in sentences if len(s.split()) > 3]  # Filter out very short sentences\n",
    "\n",
    "def extract_relevant_section(extracted_text, query):\n",
    "    \"\"\"Extract the most relevant heading and its related paragraphs.\"\"\"\n",
    "    lines = extracted_text.split(\"\\n\")\n",
    "    relevant_heading = None\n",
    "    relevant_paragraphs = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Detect headings (h1, h2, etc.)\n",
    "        if line.lower().startswith((\"h1:\", \"h2:\", \"h3:\", \"h4:\", \"h5:\", \"h6:\")):\n",
    "            relevant_heading = line  # Store the heading\n",
    "        elif relevant_heading and len(line.split()) > 5:  # Ensure it's a paragraph\n",
    "            if any(keyword in relevant_heading.lower() for keyword in query.lower().split()):\n",
    "                relevant_paragraphs.append(line)\n",
    "\n",
    "    return \"\\n\".join(relevant_paragraphs) if relevant_paragraphs else extracted_text\n",
    "\n",
    "\n",
    "def get_key_steps(sentences, max_steps=10):\n",
    "    \"\"\"Extracts action-oriented key steps with fixed emoji assignments.\"\"\"\n",
    "    steps = []\n",
    "    \n",
    "    for s in sentences:\n",
    "        for keyword, emoji in KEYWORD_EMOJI_MAP.items():\n",
    "            if keyword in s.lower():\n",
    "                steps.append(f\"{emoji} {s}\")\n",
    "                break  # Ensure a step gets only one emoji\n",
    "\n",
    "    return \"\\n\".join(steps[:max_steps]) if steps else \"üîπ No key steps available.\"\n",
    "\n",
    "def summarize_text(text, query=\" \", include_greeting=True):\n",
    "    \"\"\"Generates a formatted summary with title, key steps, and optional greetings.\"\"\"\n",
    "\n",
    "    text = extract_relevant_section(text, query)  # Extract relevant sections\n",
    "    sentences = extract_sentences(text)  # Convert to a list of sentences\n",
    "\n",
    "    if not sentences:\n",
    "        return \"No valid content found.\"\n",
    "\n",
    "    # ‚úÖ Only include greeting if allowed\n",
    "    greeting = random.choice(GREETINGS) if include_greeting else \"\"\n",
    "    closing = random.choice(CLOSINGS)\n",
    "\n",
    "    summary_paragraph = generate_summary(text, num_sentences=2)\n",
    "\n",
    "    response = PROMPT_TEMPLATE.format(\n",
    "        greeting=greeting,\n",
    "        summary=summary_paragraph,  \n",
    "        key_steps=get_key_steps(sentences),\n",
    "        closing=closing\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Remove extra spaces or unwanted greetings if not needed\n",
    "    response = response.strip()\n",
    "    if not include_greeting:\n",
    "         # Remove greeting if not required\n",
    "        response = response.replace(greeting, \"\").strip() \n",
    "         # Clean up empty lines\n",
    "        response = response.replace(\"\\n\\n\", \"\\n\") \n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_query(query):\n",
    "    \"\"\"Replaces query words with their most common synonyms for better search accuracy.\"\"\"\n",
    "    words = query.lower().split()\n",
    "    normalized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        found_synonym = False\n",
    "        for key, synonyms in QUERY_SYNONYMS.items():\n",
    "            if word in synonyms or word == key:\n",
    "                normalized_words.append(key)  # Use the standard word\n",
    "                found_synonym = True\n",
    "                break\n",
    "        \n",
    "        if not found_synonym:\n",
    "            normalized_words.append(word)  # Keep original if no synonym found\n",
    "    \n",
    "    return \" \".join(normalized_words)\n",
    "\n",
    "def adjust_response_wording(response, query):\n",
    "    \"\"\"Replaces default response wording to match the user's query phrasing.\"\"\"\n",
    "    words_in_query = query.lower().split()\n",
    "    adjusted_response = response\n",
    "\n",
    "    for key, synonyms in QUERY_SYNONYMS.items():\n",
    "        for synonym in synonyms:\n",
    "            if synonym in words_in_query:\n",
    "                # Replace the standard word in the response with the synonym found in the query\n",
    "                adjusted_response = re.sub(rf'\\b{key}\\b', synonym, adjusted_response, flags=re.IGNORECASE)\n",
    "\n",
    "    return adjusted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [11/Mar/2025 14:08:06] \"POST /chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Mar/2025 14:08:10] \"POST /chat HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import random\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Default value for FAISS search\n",
    "TOP_K = 3\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def search_faiss():\n",
    "    \"\"\"Flask API endpoint for FAISS search\"\"\"\n",
    "\n",
    "    # ‚úÖ Ensure request has Content-Type application/json\n",
    "    if request.content_type != \"application/json\":\n",
    "        return jsonify({\"error\": \"Invalid request. Content-Type must be 'application/json'\"}), 415\n",
    "\n",
    "    # ‚úÖ Handle empty request body\n",
    "    if not request.data or request.data.strip() == b'':\n",
    "        return jsonify({\"error\": \"Empty request body. Please provide valid JSON.\"}), 400\n",
    "\n",
    "    try:\n",
    "        # ‚úÖ Attempt to parse JSON safely\n",
    "        data = request.get_json()\n",
    "        if not data:\n",
    "            return jsonify({\"error\": \"Empty JSON body. Please provide valid JSON.\"}), 400\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": f\"Invalid JSON format: {str(e)}\"}), 400\n",
    "\n",
    "    # ‚úÖ Extract the \"query\" field\n",
    "    query = data.get(\"query\", \"\").strip()\n",
    "    if not query:\n",
    "        return jsonify({\"message\": \"Please enter something\"}), 400\n",
    "\n",
    "    # ‚úÖ Extract greeting and question\n",
    "    greeting, question = extract_greeting_and_question(query)\n",
    "\n",
    "    # ‚úÖ Handle special cases (Appreciation & Help requests)\n",
    "    if greeting == \"Appreciation\":\n",
    "        return jsonify({\"response\": random.choice(APPRECIATION_RESPONSES)})\n",
    "\n",
    "    if greeting == \"Help\":\n",
    "        return jsonify({\"response\": random.choice(HELP_RESPONSE)})\n",
    "\n",
    "    # ‚úÖ Handle greeting-only messages\n",
    "    if greeting and question is None:\n",
    "        return jsonify({\"response\": get_greeting_response(greeting)})\n",
    "\n",
    "    # ‚úÖ Handle cases where no valid question is found\n",
    "    if question is None:\n",
    "        return jsonify({\n",
    "            \"response\": f\"{get_greeting_response(greeting)}\\n\\nHow can I assist you today?\" if greeting else \"How can I assist you today?\"\n",
    "        })\n",
    "\n",
    "    # ‚úÖ Normalize the query for better matching\n",
    "    normalized_question = normalize_query(question)\n",
    "\n",
    "    # ‚úÖ Ensure FAISS index files exist before searching\n",
    "    if not os.path.exists(FAISS_INDEX_FILE) or not os.path.exists(EMBEDDINGS_FILE):\n",
    "        return jsonify({\"response\": \"No FAISS index found. Please build the index first.\"})\n",
    "\n",
    "    # ‚úÖ Load FAISS index and stored content\n",
    "    try:\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"response\": f\"Error loading FAISS index: {e}\"})\n",
    "\n",
    "    try:\n",
    "        content = np.load(EMBEDDINGS_FILE, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"response\": f\"Error loading stored text: {e}\"})\n",
    "\n",
    "    # ‚úÖ Convert query into an embedding and perform FAISS search\n",
    "    query_embedding = model.encode([normalized_question], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    distances, indices = index.search(query_embedding, TOP_K)\n",
    "\n",
    "    # ‚úÖ Handle case where FAISS returns no results\n",
    "    if indices is None or len(indices[0]) == 0:\n",
    "        return jsonify({\"response\": \"No relevant results found.\"})\n",
    "\n",
    "    # ‚úÖ Retrieve results based on FAISS indices\n",
    "    results = [content[i] for i in indices[0] if 0 <= i < len(content)]\n",
    "\n",
    "    # ‚úÖ Remove duplicates and short responses\n",
    "    results = list(dict.fromkeys(results))  # Remove duplicates\n",
    "    results = [r for r in results if len(r.split()) > 10]  # Ensure meaningful responses\n",
    "\n",
    "    if not results:\n",
    "        return jsonify({\"response\": \"Oops! I don't know the answer to this, Please Contact the Team\"})\n",
    "\n",
    "    # ‚úÖ Generate summary from retrieved results\n",
    "    summarized_response = summarize_text(\" \".join(results), query=normalized_question, include_greeting=(greeting is not None))\n",
    "\n",
    "    # ‚úÖ Adjust response wording based on the query\n",
    "    final_response = adjust_response_wording(summarized_response, query)\n",
    "\n",
    "    # ‚úÖ Combine greeting & final response if greeting exists\n",
    "    formatted_response = {\n",
    "        \"response\": f\"{get_greeting_response(greeting)}\\n\\n{final_response}\" if greeting else final_response\n",
    "    }\n",
    "\n",
    "    return jsonify(formatted_response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
