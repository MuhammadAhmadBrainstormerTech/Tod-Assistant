{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import nltk\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from flask import Flask, request, jsonify\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# File paths\n",
    "STORAGE_FILE = \"scraped_content.txt\"\n",
    "FAISS_INDEX_FILE = \"faiss_indexx.bin\"\n",
    "EMBEDDINGS_FILE = \"embeddingss.npy\"\n",
    "\n",
    "# Load Sentence Transformer model\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETINGS = [\n",
    "    \"Hey there! ğŸ˜Š\",  \n",
    "    \"Hello! How are you Today? ğŸ˜Š\",  \n",
    "    \"Hi! Hope you're doing great.\",  \n",
    "    \"Hey! Letâ€™s explore your questions together ğŸš€\",  \n",
    "    \"Hi there! ğŸ’¡\",  \n",
    "    \"Hello! Hope your day is going well. âœ¨\",  \n",
    "    \"HeyğŸ”\",\n",
    "    \"Hi!ğŸ‘\",  \n",
    "    \"Hey there! Ready for a quick assistance ğŸ˜Š\",  \n",
    "    \"Hello! Letâ€™s dive in! ğŸ†\",  \n",
    "    \"Hi, I'm Todd, your friendly assistant ğŸ˜Š\", \n",
    "    \"Hi there! I'm good ğŸ˜Š\",  \n",
    "    \"Doing great! How can I help? ğŸ‘\",  \n",
    "    \"I'm good, thanks for asking! What about you?\",  \n",
    "    \"Nice to meet you! How can I assist? ğŸ¤–\",  \n",
    "    \"Hope you're having a great day! â˜€ï¸\",  \n",
    "    \"Hey! What's new with you? ğŸš€\",  \n",
    "    \"It's great to see you! ğŸ˜Š\",  \n",
    "]\n",
    "\n",
    "CLOSINGS = [\n",
    "    \"Hope this helps! Let me know if you need anything else. ğŸš€\",\n",
    "    \"That's all for now! Let me know if you have more questions. ğŸ˜Š\",\n",
    "    \"Feel free to ask if you need further clarification. Have a great day! ğŸ‰\",\n",
    "    \"Let me know if I can assist you with anything else! âœ¨\",\n",
    "    \"I hope this makes things clearer. Reach out if you need more help! ğŸ‘\",\n",
    "    \"If you need any more details, just let me know! ğŸ˜Š\",\n",
    "    \"Thatâ€™s it for now! Hope you have a great day ahead. ğŸŒŸ\",\n",
    "    \"Happy to help! Let me know if you need more info. ğŸ¯\",\n",
    "    \"Hope this answered your question! Feel free to ask anything else. ğŸ”\",\n",
    "    \"Take care and let me know if you need anything else! ğŸ˜Š\"\n",
    "]\n",
    "\n",
    "KEYWORD_EMOJI_MAP = {\n",
    "    \"click\": \"ğŸ–±ï¸\", \"select\": \"âœ…\", \"navigate\": \"ğŸ§­\", \"update\": \"ğŸ”„\", \"manage\": \"ğŸ“‹\",\n",
    "    \"open\": \"ğŸ“‚\", \"go to\": \"â¡ï¸\", \"press\": \"ğŸ”˜\", \"drag\": \"ğŸ–±ï¸\", \"drop\": \"ğŸ“¥\",\n",
    "    \"choose\": \"ğŸ¯\", \"install\": \"âš™ï¸\", \"enable\": \"ğŸ”›\", \"disable\": \"ğŸš«\",\n",
    "    \"configure\": \"ğŸ”§\", \"customize\": \"ğŸ¨\"\n",
    "}\n",
    "\n",
    "APPRECIATION_RESPONSES = [\n",
    "    \"You're welcome! ğŸ˜Š Let me know if you need anything else.\",\n",
    "    \"Glad I could help! ğŸš€\",\n",
    "    \"Happy to assist! ğŸ¯\",\n",
    "    \"No problem! Let me know if you have more questions. ğŸ‘\",\n",
    "    \"Always here to help! ğŸ˜Š\",\n",
    "    \"Thanks for your kind words! Let me know if you need more info. ğŸ’¡\",\n",
    "    \"Much appreciated! If you need anything else, feel free to ask! âœ¨\"\n",
    "]\n",
    "HELP_RESPONSE = [\n",
    "    # Friendly & Encouraging Responses\n",
    "    \"Sure!! What do you need help with? ğŸ˜Š\",\n",
    "    \"Of course! I'm happy to help. What's your question? ğŸš€\",\n",
    "    \"No worries! Tell me what you need help with, and I'll do my best! âœ¨\",\n",
    "    \"Absolutely! Let me know what you're stuck on. I'll guide you. ğŸ’¡\",\n",
    "    \"You're not alone! I'm here to help. What do you need assistance with? ğŸ”\",\n",
    "\n",
    "    # Professional & Supportive Responses\n",
    "    \"I'm here to assist. Please describe your issue, and I'll help you solve it. ğŸ‘\",\n",
    "    \"I'm happy to support you. Could you provide more details about your question? ğŸ¤”\",\n",
    "    \"Iâ€™d be glad to help! Let me know what you're struggling with. ğŸ¯\",\n",
    "    \"No problem! Just let me know how I can assist you today. ğŸ”§\",\n",
    "    \"I understand! Please share the details, and I'll do my best to help. âœ…\",\n",
    "\n",
    "    # Fast & Reassuring Responses\n",
    "    \"Got it! Letâ€™s tackle this together. What do you need help with? ğŸ’ª\",\n",
    "    \"I see! Let me simplify things for you. Just tell me what you need. ğŸ“\",\n",
    "    \"Helping is what I do best! Whatâ€™s on your mind? ğŸ‰\",\n",
    "    \"I hear you! Letâ€™s find the best solution together. Tell me more. ğŸ”\",\n",
    "    \"Sure thing! Let me guide you step by step. Whatâ€™s the issue? ğŸ‘¨â€ğŸ«\",\n",
    "\n",
    "    # Problem-Solving & Guidance-Based Responses\n",
    "    \"Donâ€™t worry, weâ€™ll figure this out! Just give me the details. ğŸš€\",\n",
    "    \"Let's get this sorted! Tell me what's going on, and I'll help. ğŸ¯\",\n",
    "    \"Iâ€™m ready to assist! What specific issue are you facing? ğŸ› \",\n",
    "    \"Break it down for me! Iâ€™ll help you get to the solution. ğŸ§©\",\n",
    "    \"Happy to help! Let's find the best way forward. Whatâ€™s the challenge? âš¡\",\n",
    "\n",
    "    # Empathetic & Motivational Responses\n",
    "    \"I know how frustrating this can be! Letâ€™s work through it together. ğŸŒŸ\",\n",
    "    \"Take your time! Iâ€™ll be here to help whenever youâ€™re ready. â³\",\n",
    "    \"You're doing great! Letâ€™s make this easier. What do you need? ğŸš€\",\n",
    "    \"I totally get it! Letâ€™s get this sorted right away. ğŸ”¥\",\n",
    "    \"You're not alone in this! Let's solve it step by step. ğŸ†\"\n",
    "]\n",
    "\n",
    "GENERIC_GREETINGS = {\n",
    "    # âœ… Common English greetings (Mapped to Multiple Responses)\n",
    "    (\"hi\", \"hello\", \"hey\", \"hey there\", \"hi there\", \"greetings\", \"hello there\"): [0, 2, 8, 16], \n",
    "    (\"good morning\", \"good afternoon\", \"good evening\", \"good day\"): [5, 15, 17],  \n",
    "\n",
    "    # âœ… \"How are you?\" Variations (Multiple Responses)\n",
    "    (\"how are you\", \"howâ€™s it going\", \"how are you doing\", \"how do you do\", \n",
    "     \"how have you been\", \"howâ€™s your day\", \"howâ€™s life\", \"howâ€™s everything\"): [3, 11, 12, 13],  \n",
    "\n",
    "    # âœ… Casual slang & internet greetings (Mapped to Multiple Responses)\n",
    "    (\"wassup\", \"whatsup\", \"sup\", \"sup bro\", \"sup dude\", \"yo\", \"hiya\", \"holla\", \n",
    "     \"hey mate\", \"yo yo\", \"whatâ€™s good\", \"whatâ€™s new\", \"hey fam\", \"hey bro\", \"hey sis\", \"howâ€™s it hanging\"): [6, 7, 16],  \n",
    "\n",
    "    # âœ… Friendly reunion greetings (Mapped to Multiple Responses)\n",
    "    (\"long time no see\", \"hey friend\", \"hi buddy\", \"hello friend\", \"nice to meet you\", \"pleased to meet you\"): [8, 14, 17],  \n",
    "\n",
    "    # âœ… Formal greetings (Mapped to Multiple Responses)\n",
    "    (\"good to see you\", \"hope youâ€™re doing well\", \"it's a pleasure to meet you\"): [4, 5, 14],  \n",
    "\n",
    "    # âœ… Multilingual greetings (Mapped to Multiple Responses)\n",
    "    (\"hola\", \"bonjour\", \"ciao\", \"shalom\", \"salam\", \"aloha\", \"namaste\", \"konnichiwa\", \n",
    "     \"annyeong\", \"ni hao\", \"guten tag\", \"privet\", \"zdravstvuyte\", \"merhaba\", \"sawubona\", \"vanakkam\", \"yassas\"): [1, 9, 15],  \n",
    "\n",
    "    # âœ… Todd Introductions (Mapped to Multiple Responses)\n",
    "    (\"who are you\", \"what are you\", \"introduce yourself\", \"tell me about yourself\"): [10]  \n",
    "}\n",
    "\n",
    "\n",
    "APPRECIATION_KEYWORDS = [\n",
    "    # Common appreciation phrases\n",
    "    \"thanks\", \"ok\", \"thank you\", \"appreciate it\", \"great work\", \"nice job\",\n",
    "    \"well done\", \"awesome\", \"amazing\", \"good job\", \"fantastic\", \"love it\",\n",
    "    \"keep it up\", \"kudos\", \"much appreciated\", \"hats off\", \"respect\",\n",
    "    \n",
    "    # Expressing gratitude casually\n",
    "    \"thanks a lot\", \"many thanks\", \"thanks so much\", \"thanks a ton\",\n",
    "    \"thanks a bunch\", \"cheers\", \"big thanks\", \"huge thanks\", \"massive thanks\",\n",
    "    \"thanks buddy\", \"thanks bro\", \"thank you so much\", \"thank you tons\",\n",
    "    \n",
    "    # Formal expressions of gratitude\n",
    "    \"I truly appreciate it\", \"I'm grateful\", \"much obliged\", \"I'm in your debt\",\n",
    "    \"thank you kindly\", \"I can't thank you enough\", \"eternally grateful\",\n",
    "    \"sincere thanks\", \"profound gratitude\", \"heartfelt thanks\",\n",
    "\n",
    "    # Internet slang/modern appreciation\n",
    "    \"ty\", \"tysm\", \"thx\", \"thnx\", \"gracias\", \"danke\", \"merci\", \"arigato\",\n",
    "    \"shukran\", \"shukriya\", \"obrigado\", \"grazie\", \"dhanyavad\", \"takk\",\n",
    "    \"you rock\", \"you're the best\", \"big fan\", \"mad respect\", \"goat\",\n",
    "    \n",
    "    # Compliments & positive feedback\n",
    "    \"amazing job\", \"superb work\", \"phenomenal\", \"excellent work\",\n",
    "    \"brilliant work\", \"outstanding effort\", \"exceptional\", \"top-notch\",\n",
    "    \"terrific\", \"impressive\", \"legendary\", \"mind-blowing\"\n",
    "]\n",
    "HELP_KEYWORDS = [\n",
    "    # Direct Help Requests\n",
    "    \"help\", \"please help\", \"can you help me\", \"help me\", \"assist me\", \n",
    "    \"i need help\", \"i need assistance\", \"can you assist me\", \"help needed\",\n",
    "    \n",
    "    # Casual Help Requests\n",
    "    \"i'm stuck\", \"stuck here\", \"i can't figure this out\", \"i don't get it\",\n",
    "    \"help me out\", \"need guidance\", \"can you support\", \"support needed\",\n",
    "    \"i'm confused\", \"i need some guidance\", \"can you explain this\",\n",
    "\n",
    "    # Formal/Professional Help Requests\n",
    "    \"i have an issue\", \"i have a problem\", \"can you clarify\", \"need clarification\",\n",
    "    \"i need your help\", \"can i ask something\", \"i have a query\", \n",
    "    \"can i ask a question\", \"i need some advice\", \"can you provide guidance\",\n",
    "    \"can you help me understand this\", \"can you shed some light on this\",\n",
    "\n",
    "    # Task-Specific Help Requests\n",
    "    \"how do i do this\", \"how do i use this\", \"how do i solve this\", \n",
    "    \"what do i do next\", \"what should i do\", \"i'm not sure what to do\",\n",
    "    \"i don't know how to proceed\", \"how do i proceed\", \n",
    "    \"explain this to me\", \"guide me through this\",\n",
    "\n",
    "    # Polite Help Requests\n",
    "    \"could you help me\", \"would you mind helping me\", \"i'd appreciate your help\",\n",
    "    \"may i ask for help\", \"kindly assist me\", \"please assist me\",\n",
    "    \"i would like some help\", \"i'm seeking guidance\", \"can you lend a hand\"\n",
    "]\n",
    "# âœ… Define Query Synonym Mapping\n",
    "QUERY_SYNONYMS = {\n",
    "    \"student\": [\"kid\", \"child\", \"new student\", \"pupil\"],\n",
    "    \"add\": [\"register\", \"enroll\", \"create\"],\n",
    "    \"remove\": [\"delete\", \"erase\", \"unregister\"],\n",
    "    \"access\": [\"view\",\"go to\"],\n",
    "    \"update\": [\"edit\", \"modify\", \"change\"],\n",
    "    \"password\": [\"credentials\", \"passcode\", \"login key\"],\n",
    "    \"staff\": [\"instructor\", \"educator\", \"employee\",\"worker\"],    \n",
    "    # Dashboard-related\n",
    "    \"dashboard\": [\"panel\", \"control center\", \"admin panel\", \"overview\", \"interface\"],\n",
    "    \"company dashboard\": [\"business panel\", \"corporate overview\", \"organization dashboard\"],\n",
    "    # Attendance-related\n",
    "    \"attendance\": [\"presence\", \"check-in\", \"roll call\", \"participation\", \"time tracking\"],\n",
    "    # Branch-related\n",
    "    \"branch\": [\"division\", \"unit\", \"location\", \"office\", \"subdivision\"],\n",
    "    # Classroom-related\n",
    "    \"classroom\": [\"lecture hall\", \"learning space\", \"study room\", \"training room\"],\n",
    "    # Promotion-related\n",
    "    \"promotion\": [\"advancement\", \"upgrade\", \"progression\", \"elevation\", \"boost\"],\n",
    "    # Admission Query-related\n",
    "    \"admission query\": [\"enrollment request\", \"application inquiry\", \"registration query\", \"student admission\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index and embeddings once to optimize performance\n",
    "if os.path.exists(\"faiss_indexx.bin\") and os.path.exists(\"embeddingss.npy\"):\n",
    "    index = faiss.read_index(\"faiss_indexx.bin\")\n",
    "    content = np.load(\"embeddingss.npy\", allow_pickle=True)\n",
    "    print(\"Load Success\")\n",
    "else:\n",
    "    index = None\n",
    "    content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "{greeting}\n",
    "\n",
    "    {summary}\n",
    "ğŸ”‘ Key Steps:\n",
    "{key_steps}\n",
    "\n",
    "{closing}\n",
    "\"\"\"\n",
    "first_query = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_summary_text(text):\n",
    "    \"\"\"Cleans and refines summary text by removing unnecessary words and rewording.\"\"\"\n",
    "    text = re.sub(r'\\b(that|which|however|thus|therefore|hence|additionally|moreover|furthermore|consequently|nevertheless)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(for example|such as|including|like)\\b', 'e.g.', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "def extract_key_sentences(text, num_sentences=3):\n",
    "    \"\"\"Extracts key sentences using TF-IDF importance scoring and position-based ranking.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences) \n",
    "\n",
    "    # âœ… Compute TF-IDF scores for words\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A1  \n",
    "\n",
    "    # âœ…Rank\n",
    "    ranked_sentences = sorted(\n",
    "        enumerate(sentence_scores), key=lambda x: (x[1], -x[0]), reverse=True\n",
    "    )\n",
    "\n",
    "    # âœ…Top\n",
    "    best_sentences = [sentences[idx] for idx, _ in ranked_sentences[:num_sentences]]\n",
    "    return \" \".join(best_sentences)\n",
    "\n",
    "def generate_summary(text, num_sentences=3):\n",
    "    \"\"\"Generates an improved summary using LSA summarization.\"\"\"\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "\n",
    "    summary_sentences = summarizer(parser.document, num_sentences)\n",
    "    summary_text = \" \".join(str(sentence) for sentence in summary_sentences)\n",
    "\n",
    "    return summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_repeated_chars(text):\n",
    "    \"\"\"Reduces excessive character repetition but allows up to 3 consecutive occurrences.\"\"\"\n",
    "    return re.sub(r'(.)\\1{3,}', r'\\1\\1\\1', text) \n",
    "\n",
    "def safe_extract_one(text, choices):\n",
    "    \"\"\"Handles cases where fuzzy matching might return None.\"\"\"\n",
    "    result = process.extractOne(text, choices, scorer=fuzz.ratio)\n",
    "    return result if result else (\"\", 0)  # Avoid unpacking error\n",
    "\n",
    "def extract_greeting_and_question(query):\n",
    "    \"\"\"Detects if the full query is a greeting, appreciation, or an actual question using fuzzy matching.\"\"\"\n",
    "\n",
    "    # âœ… Remove punctuation and convert to lowercase for better matching\n",
    "    query_cleaned = re.sub(r'[^\\w\\s]', '', query.lower().strip())\n",
    "\n",
    "    # âœ… Normalize\n",
    "    query_cleaned = reduce_repeated_chars(query_cleaned)\n",
    "\n",
    "    words = word_tokenize(query_cleaned)\n",
    "    full_query = \" \".join(words) \n",
    "    \n",
    "    \n",
    "    # âœ…Closest Match\n",
    "    best_greeting_match, greeting_score = safe_extract_one(full_query, [item for subset in GENERIC_GREETINGS.keys() for item in subset])\n",
    "    best_appreciation_match, appreciation_score = process.extractOne(full_query, APPRECIATION_KEYWORDS, scorer=fuzz.ratio)\n",
    "    best_help_match, help_score = process.extractOne(full_query, HELP_KEYWORDS, scorer=fuzz.ratio)\n",
    "\n",
    "    # âœ…Greeting\n",
    "    if greeting_score >= 70:\n",
    "        return best_greeting_match.capitalize(), None  \n",
    "\n",
    "    # âœ…Appreciation\n",
    "    if appreciation_score >= 70:\n",
    "        return \"Appreciation\", None  \n",
    "    \n",
    "    if help_score >= 70:\n",
    "        return \"Help\", None\n",
    "\n",
    "    # âœ…Mixed Greeting\n",
    "    for phrase_group in GENERIC_GREETINGS.keys():\n",
    "        for phrase in phrase_group:\n",
    "            if full_query.startswith(phrase + \" \"): \n",
    "                greeting = phrase.capitalize()\n",
    "                # âœ… Remove greeting from query\n",
    "                cleaned_query = full_query[len(phrase):].strip()  \n",
    "                 # âœ… Return cleaned query or None\n",
    "                return greeting, cleaned_query if cleaned_query else None \n",
    "\n",
    "    return None, query \n",
    "\n",
    "def get_greeting_response(user_input):\n",
    "    \"\"\"Returns an appropriate greeting response based on user input.\"\"\"\n",
    "    user_input = user_input.lower()\n",
    "\n",
    "    for key_set, response in GENERIC_GREETINGS.items():\n",
    "        if user_input in key_set:\n",
    "            # âœ… Picks random index, fetches from GREETINGS\n",
    "            return GREETINGS[random.choice(response)]  \n",
    "\n",
    "    return \"Hey! How can I assist you today? ğŸ˜Š\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Removes extra spaces, fixes phrasing, and makes text more readable.\"\"\"\n",
    "    # Remove extra spaces/newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    text = re.sub(r'\\bprovides quick access to\\b', 'lets you quickly access', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\ballows managing\\b', 'helps manage', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bclick on\\b', 'select', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def extract_sentences(text):\n",
    "    \"\"\"Splits text into meaningful sentences.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Proper sentence splitting\n",
    "    return [s.strip() for s in sentences if len(s.split()) > 3]  # Filter out very short sentences\n",
    "\n",
    "def extract_relevant_section(extracted_text, query):\n",
    "    \"\"\"Extract the most relevant heading and its related paragraphs.\"\"\"\n",
    "    lines = extracted_text.split(\"\\n\")\n",
    "    relevant_heading = None\n",
    "    relevant_paragraphs = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Detect headings (h1, h2, etc.)\n",
    "        if line.lower().startswith((\"h1:\", \"h2:\", \"h3:\", \"h4:\", \"h5:\", \"h6:\")):\n",
    "            relevant_heading = line  # Store the heading\n",
    "        elif relevant_heading and len(line.split()) > 5:  # Ensure it's a paragraph\n",
    "            if any(keyword in relevant_heading.lower() for keyword in query.lower().split()):\n",
    "                relevant_paragraphs.append(line)\n",
    "\n",
    "    return \"\\n\".join(relevant_paragraphs) if relevant_paragraphs else extracted_text\n",
    "\n",
    "\n",
    "def get_key_steps(sentences, max_steps=10):\n",
    "    \"\"\"Extracts action-oriented key steps with fixed emoji assignments.\"\"\"\n",
    "    steps = []\n",
    "    \n",
    "    for s in sentences:\n",
    "        for keyword, emoji in KEYWORD_EMOJI_MAP.items():\n",
    "            if keyword in s.lower():\n",
    "                steps.append(f\"{emoji} {s}\")\n",
    "                break  # Ensure a step gets only one emoji\n",
    "\n",
    "    return \"\\n\".join(steps[:max_steps]) if steps else \"ğŸ”¹ No key steps available.\"\n",
    "\n",
    "def summarize_text(text, query=\" \", include_greeting=True):\n",
    "    \"\"\"Generates a formatted summary with title, key steps, and optional greetings.\"\"\"\n",
    "\n",
    "    text = extract_relevant_section(text, query)  # Extract relevant sections\n",
    "    sentences = extract_sentences(text)  # Convert to a list of sentences\n",
    "\n",
    "    if not sentences:\n",
    "        return \"No valid content found.\"\n",
    "\n",
    "    # âœ… Only include greeting if allowed\n",
    "    greeting = random.choice(GREETINGS) if include_greeting else \"\"\n",
    "    closing = random.choice(CLOSINGS)\n",
    "\n",
    "    summary_paragraph = generate_summary(text, num_sentences=2)\n",
    "\n",
    "    response = PROMPT_TEMPLATE.format(\n",
    "        greeting=greeting,\n",
    "        summary=summary_paragraph,  \n",
    "        key_steps=get_key_steps(sentences),\n",
    "        closing=closing\n",
    "    )\n",
    "\n",
    "    # âœ… Remove extra spaces or unwanted greetings if not needed\n",
    "    response = response.strip()\n",
    "    if not include_greeting:\n",
    "         # Remove greeting if not required\n",
    "        response = response.replace(greeting, \"\").strip() \n",
    "         # Clean up empty lines\n",
    "        response = response.replace(\"\\n\\n\", \"\\n\") \n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_query(query):\n",
    "    \"\"\"Replaces query words with their most common synonyms for better search accuracy.\"\"\"\n",
    "    words = query.lower().split()\n",
    "    normalized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        found_synonym = False\n",
    "        for key, synonyms in QUERY_SYNONYMS.items():\n",
    "            if word in synonyms or word == key:\n",
    "                normalized_words.append(key)  # Use the standard word\n",
    "                found_synonym = True\n",
    "                break\n",
    "        \n",
    "        if not found_synonym:\n",
    "            normalized_words.append(word)  # Keep original if no synonym found\n",
    "    \n",
    "    return \" \".join(normalized_words)\n",
    "\n",
    "def adjust_response_wording(response, query):\n",
    "    \"\"\"Replaces default response wording to match the user's query phrasing.\"\"\"\n",
    "    words_in_query = query.lower().split()\n",
    "    adjusted_response = response\n",
    "\n",
    "    for key, synonyms in QUERY_SYNONYMS.items():\n",
    "        for synonym in synonyms:\n",
    "            if synonym in words_in_query:\n",
    "                # Replace the standard word in the response with the synonym found in the query\n",
    "                adjusted_response = re.sub(rf'\\b{key}\\b', synonym, adjusted_response, flags=re.IGNORECASE)\n",
    "\n",
    "    return adjusted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def search_faiss():\n",
    "    \"\"\"Flask API endpoint for FAISS search\"\"\"\n",
    "\n",
    "    # âœ… Get JSON input from user request\n",
    "    data = request.get_json()\n",
    "    query = data.get(\"query\", \"\")\n",
    "    top_k = data.get(\"top_k\", 1)  # Default to 1 if not provided\n",
    "\n",
    "    if not query:\n",
    "        return jsonify({\"error\": \"Query parameter is required\"}), 400\n",
    "\n",
    "    global first_query  # Track if it's the first query\n",
    "\n",
    "    # âœ… Extract greeting, appreciation, and actual query\n",
    "    greeting, question = extract_greeting_and_question(query)\n",
    "\n",
    "    # âœ… Debugging print to check extracted values\n",
    "    print(f\"DEBUG - Greeting: {greeting}, Question: {question}\")\n",
    "\n",
    "    # âœ… If it's an appreciation message, return a polite response\n",
    "    if greeting == \"Appreciation\":\n",
    "        return jsonify({\"response\": random.choice(APPRECIATION_RESPONSES)})\n",
    "    \n",
    "    if greeting == \"Help\":\n",
    "        return jsonify({\"response\": random.choice(HELP_RESPONSE)})\n",
    "\n",
    "    # âœ… Handle greetings: If only a greeting exists, return a greeting response\n",
    "    if greeting and question is None:\n",
    "        return jsonify({\"response\": get_greeting_response(greeting)})\n",
    "\n",
    "    # âœ… If both a greeting AND a question exist, return both\n",
    "    greeting_text = get_greeting_response(greeting) if greeting and not question else \"\"\n",
    "\n",
    "    # âœ… If no question is found, return a generic response\n",
    "    if question is None:\n",
    "        return jsonify({\"response\": f\"{greeting_text}\\n\\nHow can I assist you today?\" if greeting_text else \"How can I assist you today?\"})\n",
    "\n",
    "    # âœ… Normalize the query to handle variations\n",
    "    normalized_question = normalize_query(question)\n",
    "    print(f\"DEBUG - Normalized Question: {normalized_question}\")\n",
    "\n",
    "    # âœ… Ensure FAISS index exists\n",
    "    if not os.path.exists(FAISS_INDEX_FILE) or not os.path.exists(EMBEDDINGS_FILE):\n",
    "        return jsonify({\"response\": \"No FAISS index found. Please build the index first.\"})\n",
    "\n",
    "    try:\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"response\": f\"Error loading FAISS index: {e}\"})\n",
    "\n",
    "    try:\n",
    "        content = np.load(EMBEDDINGS_FILE, allow_pickle=True)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"response\": f\"Error loading stored text: {e}\"})\n",
    "\n",
    "    # Convert query into an embedding\n",
    "    query_embedding = model.encode([normalized_question], convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # Perform FAISS search\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # âœ… Check if FAISS returned valid results\n",
    "    if indices is None or len(indices[0]) == 0:\n",
    "        return jsonify({\"response\": \"No relevant results found.\"})\n",
    "\n",
    "    results = [content[i] for i in indices[0] if 0 <= i < len(content)]\n",
    "\n",
    "    # âœ… Remove duplicates & short responses\n",
    "    results = list(dict.fromkeys(results))  # Remove duplicates\n",
    "    results = [r for r in results if len(r.split()) > 10]  # Ensure meaningful text\n",
    "\n",
    "    if not results:\n",
    "        return jsonify({\"response\": \"No relevant results found.\"})\n",
    "\n",
    "    # âœ… Generate summary\n",
    "    summarized_response = summarize_text(\" \".join(results), query=normalized_question, include_greeting=(greeting is not None))\n",
    "\n",
    "    # âœ… Adjust response wording based on query\n",
    "    final_response = adjust_response_wording(summarized_response, query)\n",
    "\n",
    "    # âœ… Combine greeting & final response if greeting exists\n",
    "    return jsonify({\"response\": f\"{greeting_text}\\n\\n{final_response}\" if greeting_text else final_response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
